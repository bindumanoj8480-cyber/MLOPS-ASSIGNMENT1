{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7l6QVFwxk76"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Single-file, reproducible pipeline:\n",
        "- Load + clean UCI Heart Disease (id=45) via ucimlrepo\n",
        "- EDA plots (histograms, correlation heatmap, class balance)\n",
        "- Preprocessing (scaling + one-hot)\n",
        "- Train + tune Logistic Regression and Random Forest\n",
        "- Cross-validation metrics (accuracy, precision, recall, ROC-AUC)\n",
        "- MLflow experiment tracking (params, metrics, artifacts, models)\n",
        "- Persist final model (pickle)\n",
        "- FastAPI serving with /predict and /metrics (Prometheus)\n",
        "- Run modes: train | serve\n",
        "\n",
        "Usage:\n",
        "  python heart_pipeline.py --mode train\n",
        "  python heart_pipeline.py --mode serve --model_path data/final_model.pkl --host 0.0.0.0 --port 8000\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import joblib\n",
        "import argparse\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from typing import List\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# MLflow\n",
        "import mlflow\n",
        "from mlflow.models.signature import infer_signature\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate, GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, roc_auc_score, make_scorer\n",
        ")\n",
        "\n",
        "# FastAPI\n",
        "from fastapi import FastAPI, Response\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "\n",
        "# Prometheus metrics\n",
        "from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration and constants\n",
        "# -----------------------------\n",
        "\n",
        "CATEGORICAL_COLS: List[str] = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"thal\", \"ca\"]\n",
        "NUMERIC_COLS: List[str] = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
        "TARGET_COL: str = \"num\"  # UCI Heart Disease 'num' (0..4). We'll binarize to 0/1.\n",
        "\n",
        "SCORERS = {\n",
        "    \"accuracy\": make_scorer(accuracy_score),\n",
        "    \"precision\": make_scorer(precision_score),\n",
        "    \"recall\": make_scorer(recall_score),\n",
        "    \"roc_auc\": make_scorer(roc_auc_score),\n",
        "}\n",
        "\n",
        "FEATURE_ORDER = [\n",
        "    \"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\n",
        "    \"thalach\",\"exang\",\"oldpeak\",\"slope\",\"ca\",\"thal\"\n",
        "]\n",
        "\n",
        "EXPERIMENT_NAME = \"heart_disease_classification\"\n",
        "EDA_DIR = \"data\"\n",
        "FINAL_MODEL_PATH = os.path.join(EDA_DIR, \"final_model.pkl\")\n",
        "FINAL_CHOICE_PATH = os.path.join(EDA_DIR, \"final_choice.json\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Data loading and cleaning\n",
        "# -----------------------------\n",
        "\n",
        "def load_uci_heart() -> pd.DataFrame:\n",
        "    heart = fetch_ucirepo(id=45)\n",
        "    X = heart.data.features.copy()\n",
        "    y = heart.data.targets.copy()\n",
        "    df = pd.concat([X, y], axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_heart(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "\n",
        "    if TARGET_COL not in df.columns:\n",
        "        raise ValueError(f\"Target column '{TARGET_COL}' not found in dataset.\")\n",
        "\n",
        "    df[TARGET_COL] = (df[TARGET_COL] > 0).astype(int)\n",
        "    df = df.replace(\"?\", np.nan)\n",
        "\n",
        "    # coerce numeric\n",
        "    for col in NUMERIC_COLS + [\"ca\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    # impute\n",
        "    for col in df.columns:\n",
        "        if col == TARGET_COL:\n",
        "            continue\n",
        "        if df[col].dtype.kind in \"biufc\":\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "        else:\n",
        "            df[col] = df[col].fillna(df[col].mode().iloc[0])\n",
        "    return df\n",
        "\n",
        "\n",
        "def split_X_y(df: pd.DataFrame):\n",
        "    X = df.drop(columns=[TARGET_COL])\n",
        "    y = df[TARGET_COL]\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# EDA visualizations\n",
        "# -----------------------------\n",
        "\n",
        "def plot_histograms(df: pd.DataFrame, numeric_cols: List[str], out_path: str):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    df[numeric_cols].hist(bins=20, figsize=(12, 8))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_correlation_heatmap(df: pd.DataFrame, numeric_cols: List[str], out_path: str):\n",
        "    corr = df[numeric_cols].corr()\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_class_balance(y: pd.Series, out_path: str):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.countplot(x=y)\n",
        "    plt.title(\"Class Balance (0: no disease, 1: disease)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Feature preprocessing\n",
        "# -----------------------------\n",
        "\n",
        "def build_preprocessing() -> ColumnTransformer:\n",
        "    numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
        "    categorical_transformer = Pipeline(steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_transformer, NUMERIC_COLS),\n",
        "            (\"cat\", categorical_transformer, CATEGORICAL_COLS),\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Models, tuning, evaluation\n",
        "# -----------------------------\n",
        "\n",
        "def build_logreg_pipeline() -> Pipeline:\n",
        "    preprocessor = build_preprocessing()\n",
        "    clf = LogisticRegression(max_iter=200, solver=\"liblinear\", class_weight=\"balanced\")\n",
        "    pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", clf)])\n",
        "    return pipe\n",
        "\n",
        "\n",
        "def build_rf_pipeline() -> Pipeline:\n",
        "    preprocessor = build_preprocessing()\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=200, max_depth=None, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=42\n",
        "    )\n",
        "    pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", clf)])\n",
        "    return pipe\n",
        "\n",
        "\n",
        "def tune_logreg(X, y) -> GridSearchCV:\n",
        "    pipe = build_logreg_pipeline()\n",
        "    param_grid = {\n",
        "        \"clf__C\": [0.1, 1.0, 10.0],\n",
        "        \"clf__penalty\": [\"l1\", \"l2\"],\n",
        "    }\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    grid = GridSearchCV(pipe, param_grid, scoring=\"roc_auc\", cv=cv, n_jobs=-1)\n",
        "    grid.fit(X, y)\n",
        "    return grid\n",
        "\n",
        "\n",
        "def tune_rf(X, y) -> GridSearchCV:\n",
        "    pipe = build_rf_pipeline()\n",
        "    param_grid = {\n",
        "        \"clf__n_estimators\": [200, 400],\n",
        "        \"clf__max_depth\": [None, 8, 16],\n",
        "        \"clf__min_samples_split\": [2, 5],\n",
        "    }\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    grid = GridSearchCV(pipe, param_grid, scoring=\"roc_auc\", cv=cv, n_jobs=-1)\n",
        "    grid.fit(X, y)\n",
        "    return grid\n",
        "\n",
        "\n",
        "def evaluate_cv(pipe: Pipeline, X, y):\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    results = cross_validate(pipe, X, y, cv=cv, scoring=SCORERS, return_train_score=False)\n",
        "    return {k: float(np.mean(v)) for k, v in results.items() if \"test_\" in k}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# MLflow helpers\n",
        "# -----------------------------\n",
        "\n",
        "def init_mlflow(experiment_name=EXPERIMENT_NAME, tracking_uri=None):\n",
        "    if tracking_uri:\n",
        "        mlflow.set_tracking_uri(tracking_uri)\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "\n",
        "def log_model_with_signature(model, X_sample, model_name=\"model\"):\n",
        "    try:\n",
        "        outputs = model.predict_proba(X_sample) if hasattr(model, \"predict_proba\") else model.predict(X_sample)\n",
        "        signature = infer_signature(X_sample, outputs)\n",
        "        mlflow.sklearn.log_model(model, artifact_path=model_name, signature=signature)\n",
        "    except Exception:\n",
        "        mlflow.sklearn.log_model(model, artifact_path=model_name)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Training entrypoint\n",
        "# -----------------------------\n",
        "\n",
        "def run_training():\n",
        "    os.makedirs(EDA_DIR, exist_ok=True)\n",
        "    init_mlflow(EXPERIMENT_NAME)\n",
        "\n",
        "    df = load_uci_heart()\n",
        "    df = clean_heart(df)\n",
        "    X, y = split_X_y(df)\n",
        "\n",
        "    # EDA artifacts\n",
        "    hist_path = os.path.join(EDA_DIR, \"histograms.png\")\n",
        "    corr_path = os.path.join(EDA_DIR, \"corr_heatmap.png\")\n",
        "    class_path = os.path.join(EDA_DIR, \"class_balance.png\")\n",
        "\n",
        "    plot_histograms(df, NUMERIC_COLS, out_path=hist_path)\n",
        "    plot_correlation_heatmap(df, NUMERIC_COLS, out_path=corr_path)\n",
        "    plot_class_balance(y, out_path=class_path)\n",
        "\n",
        "    # Baselines\n",
        "    with mlflow.start_run(run_name=\"baseline_logreg\"):\n",
        "        pipe_lr = build_logreg_pipeline()\n",
        "        metrics_lr = evaluate_cv(pipe_lr, X, y)\n",
        "        mlflow.log_params({\"model\": \"logreg_baseline\"})\n",
        "        mlflow.log_metrics({k.replace(\"test_\", \"\"): v for k, v in metrics_lr.items()})\n",
        "        mlflow.log_artifact(hist_path)\n",
        "        mlflow.log_artifact(corr_path)\n",
        "        mlflow.log_artifact(class_path)\n",
        "\n",
        "    with mlflow.start_run(run_name=\"baseline_rf\"):\n",
        "        pipe_rf = build_rf_pipeline()\n",
        "        metrics_rf = evaluate_cv(pipe_rf, X, y)\n",
        "        mlflow.log_params({\"model\": \"rf_baseline\"})\n",
        "        mlflow.log_metrics({k.replace(\"test_\", \"\"): v for k, v in metrics_rf.items()})\n",
        "\n",
        "    # Tuning\n",
        "    with mlflow.start_run(run_name=\"tuned_logreg\"):\n",
        "        grid_lr = tune_logreg(X, y)\n",
        "        best_lr = grid_lr.best_estimator_\n",
        "        mlflow.log_params({\"model\": \"logreg_tuned\", **grid_lr.best_params_})\n",
        "        metrics_lr_tuned = evaluate_cv(best_lr, X, y)\n",
        "        mlflow.log_metrics({k.replace(\"test_\", \"\"): v for k, v in metrics_lr_tuned.items()})\n",
        "        log_model_with_signature(best_lr, X.iloc[:10], \"model\")\n",
        "\n",
        "    with mlflow.start_run(run_name=\"tuned_rf\"):\n",
        "        grid_rf = tune_rf(X, y)\n",
        "        best_rf = grid_rf.best_estimator_\n",
        "        mlflow.log_params({\"model\": \"rf_tuned\", **grid_rf.best_params_})\n",
        "        metrics_rf_tuned = evaluate_cv(best_rf, X, y)\n",
        "        mlflow.log_metrics({k.replace(\"test_\", \"\"): v for k, v in metrics_rf_tuned.items()})\n",
        "        log_model_with_signature(best_rf, X.iloc[:10], \"model\")\n",
        "\n",
        "    # Choose final based on ROC-AUC\n",
        "    auc_lr = metrics_lr_tuned[\"test_roc_auc\"]\n",
        "    auc_rf = metrics_rf_tuned[\"test_roc_auc\"]\n",
        "    final_model = best_rf if auc_rf >= auc_lr else best_lr\n",
        "    choice = \"rf\" if final_model is best_rf else \"logreg\"\n",
        "    with open(FINAL_CHOICE_PATH, \"w\") as f:\n",
        "        json.dump({\"final_model\": choice, \"roc_auc\": {\"rf\": auc_rf, \"logreg\": auc_lr}}, f)\n",
        "\n",
        "    joblib.dump(final_model, FINAL_MODEL_PATH)\n",
        "    print(f\"Training complete. Final model: {choice}. Saved to {FINAL_MODEL_PATH}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Serving API (FastAPI)\n",
        "# -----------------------------\n",
        "\n",
        "class HeartInput(BaseModel):\n",
        "    age: float\n",
        "    sex: int\n",
        "    cp: int\n",
        "    trestbps: float\n",
        "    chol: float\n",
        "    fbs: int\n",
        "    restecg: int\n",
        "    thalach: float\n",
        "    exang: int\n",
        "    oldpeak: float\n",
        "    slope: int\n",
        "    ca: float\n",
        "    thal: int\n",
        "\n",
        "\n",
        "class PredictResponse(BaseModel):\n",
        "    prediction: int\n",
        "    confidence: float\n",
        "    latency_ms: float\n",
        "\n",
        "\n",
        "REQUEST_COUNT = Counter(\"requests_total\", \"Total API requests\", [\"endpoint\", \"method\", \"status\"])\n",
        "LATENCY = Histogram(\"request_latency_ms\", \"Request latency in ms\", buckets=[10,50,100,200,500,1000])\n",
        "\n",
        "def create_app(model_path: str) -> FastAPI:\n",
        "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
        "    logger = logging.getLogger(\"api\")\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "    model = joblib.load(model_path)\n",
        "    app = FastAPI(title=\"Heart Disease Classifier API\")\n",
        "\n",
        "    @app.get(\"/metrics\")\n",
        "    def metrics():\n",
        "        return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)\n",
        "\n",
        "    @app.post(\"/predict\", response_model=PredictResponse)\n",
        "    def predict(input: HeartInput):\n",
        "        start = time.time()\n",
        "        x = np.array([[getattr(input, f) for f in FEATURE_ORDER]])\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            proba = float(model.predict_proba(x)[0, 1])\n",
        "        else:\n",
        "            # Fallback (rare): use decision_function or predict\n",
        "            try:\n",
        "                proba = float(model.decision_function(x))\n",
        "            except Exception:\n",
        "                proba = float(model.predict(x)[0])\n",
        "        pred = int(proba >= 0.5)\n",
        "        latency = (time.time() - start) * 1000\n",
        "        LATENCY.observe(latency)\n",
        "        REQUEST_COUNT.labels(endpoint=\"/predict\", method=\"POST\", status=\"200\").inc()\n",
        "        logger.info(f\"predict request: pred={pred} proba={proba:.4f} latency_ms={latency:.2f}\")\n",
        "        return PredictResponse(prediction=pred, confidence=proba, latency_ms=latency)\n",
        "\n",
        "    return app\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# CLI entrypoint\n",
        "# -----------------------------\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Heart Disease ML pipeline and API (single-file).\")\n",
        "    parser.add_argument(\"--mode\", choices=[\"train\", \"serve\"], required=True, help=\"Run training or serve API.\")\n",
        "    parser.add_argument(\"--model_path\", default=FINAL_MODEL_PATH, help=\"Path to saved model (for serving).\")\n",
        "    parser.add_argument(\"--host\", default=\"0.0.0.0\", help=\"API host (serve mode).\")\n",
        "    parser.add_argument(\"--port\", type=int, default=8000, help=\"API port (serve mode).\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    if args.mode == \"train\":\n",
        "        run_training()\n",
        "        print(\"Tip: launch MLflow UI with: mlflow ui --port 5000\")\n",
        "    elif args.mode == \"serve\":\n",
        "        app = create_app(args.model_path)\n",
        "        uvicorn.run(app, host=args.host, port=args.port)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}